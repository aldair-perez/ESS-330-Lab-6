[
  {
    "objectID": "Lab_6.html",
    "href": "Lab_6.html",
    "title": "Lab_6",
    "section": "",
    "text": "root  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n               'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "Lab_6.html#model-building",
    "href": "Lab_6.html#model-building",
    "title": "Lab_6",
    "section": "Model Building",
    "text": "Model Building\n\nset.seed(123)\ncamels &lt;- camels |&gt;\n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt;\n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  scale_color_gradient2(low = \"red\", mid = \"orange\", high = \"yellow\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train) \n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\ninstall.packages(\"ranger\")\n\nInstalling package into 'C:/Users/aldai/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'ranger' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\aldai\\AppData\\Local\\Temp\\Rtmp6nuWib\\downloaded_packages\n\nlibrary(ranger)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2"
  },
  {
    "objectID": "Lab_6.html#through-evaluation-of-the-models-i-have-maderandom-forest-is-the-best-to-use-due-to-its-simplicity-and-works-with-multiple-decision-trees-as-well-as-being-accurate.",
    "href": "Lab_6.html#through-evaluation-of-the-models-i-have-maderandom-forest-is-the-best-to-use-due-to-its-simplicity-and-works-with-multiple-decision-trees-as-well-as-being-accurate.",
    "title": "Lab_6",
    "section": "Through evaluation of the models I have made,random forest is the best to use due to its simplicity and works with multiple decision trees, as well as being accurate.",
    "text": "Through evaluation of the models I have made,random forest is the best to use due to its simplicity and works with multiple decision trees, as well as being accurate."
  },
  {
    "objectID": "Lab_6.html#data-spliting",
    "href": "Lab_6.html#data-spliting",
    "title": "Lab_6",
    "section": "Data Spliting",
    "text": "Data Spliting\n\nset.seed(69)\ndata_splits &lt;- initial_split(camels, prop = 0.75)\ndata_train &lt;- training(data_splits)\ndata_test &lt;- testing(data_splits)\n\ndata_cv &lt;- vfold_cv(data_train, v = 10)"
  },
  {
    "objectID": "Lab_6.html#recipe",
    "href": "Lab_6.html#recipe",
    "title": "Lab_6",
    "section": "Recipe",
    "text": "Recipe\n\nrec2 &lt;- recipe(logQmean ~ p_mean + baseflow_index, data = data_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ p_mean:baseflow_index) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n\ncamels |&gt;\n  select(p_mean, runoff_ratio, baseflow_index) |&gt;\n  drop_na() |&gt;\n  cor()\n\n                  p_mean runoff_ratio baseflow_index\np_mean         1.0000000    0.7088641      0.1602259\nrunoff_ratio   0.7088641    1.0000000      0.3452424\nbaseflow_index 0.1602259    0.3452424      1.0000000"
  },
  {
    "objectID": "Lab_6.html#i-chose-p_mean-since-mean-daily-precipiation-can-influence-streamflow.-runoff_ratio-is-the-next-predictor-since-it-helps-us-find-out-how-much-of-the-precipitation-turns-into-streamflow.-finally-baseflow_index-tells-us-how-much-of-the-streamflow-is-from-groundwater.",
    "href": "Lab_6.html#i-chose-p_mean-since-mean-daily-precipiation-can-influence-streamflow.-runoff_ratio-is-the-next-predictor-since-it-helps-us-find-out-how-much-of-the-precipitation-turns-into-streamflow.-finally-baseflow_index-tells-us-how-much-of-the-streamflow-is-from-groundwater.",
    "title": "Lab_6",
    "section": "I chose p_mean since mean daily precipiation can influence streamflow. Runoff_ratio is the next predictor since it helps us find out how much of the precipitation turns into streamflow. Finally, baseflow_index tells us how much of the streamflow is from groundwater.",
    "text": "I chose p_mean since mean daily precipiation can influence streamflow. Runoff_ratio is the next predictor since it helps us find out how much of the precipitation turns into streamflow. Finally, baseflow_index tells us how much of the streamflow is from groundwater."
  },
  {
    "objectID": "Lab_6.html#define-models",
    "href": "Lab_6.html#define-models",
    "title": "Lab_6",
    "section": "Define Models",
    "text": "Define Models\n\nrandomly_forest &lt;- rand_forest() |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\n\nboosted_model &lt;- boost_tree() |&gt;\n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n\n\nsvm_model &lt;- svm_poly() |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "Lab_6.html#workflow-set",
    "href": "Lab_6.html#workflow-set",
    "title": "Lab_6",
    "section": "Workflow Set",
    "text": "Workflow Set\n\nrandfore_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec2) %&gt;%\n  add_model(randomly_forest) %&gt;%\n  fit(data = data_train)\n\n\nboosted_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec2) %&gt;%\n  add_model(boosted_model) %&gt;%\n  fit(data = data_train)\n\n\ninstall.packages(\"kernlab\")\n\nInstalling package into 'C:/Users/aldai/AppData/Local/R/win-library/4.4'\n(as 'lib' is unspecified)\n\n\npackage 'kernlab' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'kernlab'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\aldai\\AppData\\Local\\R\\win-library\\4.4\\00LOCK\\kernlab\\libs\\x64\\kernlab.dll\nto C:\\Users\\aldai\\AppData\\Local\\R\\win-library\\4.4\\kernlab\\libs\\x64\\kernlab.dll:\nPermission denied\n\n\nWarning: restored 'kernlab'\n\n\n\nThe downloaded binary packages are in\n    C:\\Users\\aldai\\AppData\\Local\\Temp\\Rtmp6nuWib\\downloaded_packages\n\nlibrary(kernlab)\n\n\nAttaching package: 'kernlab'\n\n\nThe following object is masked from 'package:modeltime':\n\n    error\n\n\nThe following object is masked from 'package:dials':\n\n    buffer\n\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\nsvm_workflow &lt;- workflow() %&gt;%\n  add_recipe(rec2) %&gt;%\n  add_model(svm_model) %&gt;%\n  fit(data = data_train)\n\n Setting default kernel parameters  \n\n\n\nmodel_set &lt;- workflow_set(\n  preproc = list(\"recipe\" = rec2),\n  models = list(\n    rf = randomly_forest,                  \n    boosted = boosted_model,        \n    svm = svm_model))"
  },
  {
    "objectID": "Lab_6.html#evaluation",
    "href": "Lab_6.html#evaluation",
    "title": "Lab_6",
    "section": "Evaluation",
    "text": "Evaluation\n\nrandfore_resamples &lt;- fit_resamples(randfore_workflow, resamples = data_cv)\nboosted_resamples &lt;- fit_resamples(boosted_workflow, resamples = data_cv)\nsvm_resamples &lt;- fit_resamples(svm_workflow, resamples = data_cv)\n\n\nrandfore_metrics &lt;- collect_metrics(randfore_resamples)\nboosted_metrics &lt;- collect_metrics(boosted_resamples)\nsvm_metrics &lt;- collect_metrics(svm_resamples)\n\n\nall_metrics &lt;- bind_rows(\n  randfore_metrics %&gt;% mutate(model = \"Random Forest\"),\n  boosted_metrics %&gt;% mutate(model = \"Boosted\"),\n  svm_metrics %&gt;% mutate(model = \"SVM\")\n)\n\n\nranked_models &lt;- all_metrics %&gt;%\n  arrange(mean) %&gt;%\n  select(model, mean)\n\n\nggplot(ranked_models, aes(x = model, y = mean, fill = model)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Comparison of Model Performance (Mean Metric)\", \n       y = \"Mean RMSE (or another metric)\",\n       x = \"Model\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nranked_models %&gt;%\n     arrange(mean)\n\n# A tibble: 6 × 2\n  model          mean\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Random Forest 0.497\n2 SVM           0.515\n3 Boosted       0.546\n4 Boosted       0.785\n5 SVM           0.810\n6 Random Forest 0.820\n\nprint(ranked_models)\n\n# A tibble: 6 × 2\n  model          mean\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Random Forest 0.497\n2 SVM           0.515\n3 Boosted       0.546\n4 Boosted       0.785\n5 SVM           0.810\n6 Random Forest 0.820"
  },
  {
    "objectID": "Lab_6.html#the-best-model-is-boosted-because-its-in-the-middle-in-mean-and-the-difference-between-the-2-boosted-models-is-the-smallest.",
    "href": "Lab_6.html#the-best-model-is-boosted-because-its-in-the-middle-in-mean-and-the-difference-between-the-2-boosted-models-is-the-smallest.",
    "title": "Lab_6",
    "section": "The best model is boosted because it’s in the middle in mean and the difference between the 2 boosted models is the smallest.",
    "text": "The best model is boosted because it’s in the middle in mean and the difference between the 2 boosted models is the smallest."
  },
  {
    "objectID": "Lab_6.html#extact-and-evaluate",
    "href": "Lab_6.html#extact-and-evaluate",
    "title": "Lab_6",
    "section": "Extact and Evaluate",
    "text": "Extact and Evaluate\n\nboosted_workflow &lt;- workflow() %&gt;%\n  add_model(boosted_model) %&gt;%  \n  add_recipe(rec2) %&gt;%\n  fit(data = data_train)\n\n\naugmented_data &lt;- augment(boosted_workflow, new_data = data_test)\nhead(augmented_data)\n\n# A tibble: 6 × 60\n  .pred gauge_id p_mean pet_mean p_seasonality frac_snow aridity high_prec_freq\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 0.348 01054200   4.07     2.13       0.105       0.300   0.523           17.5\n2 0.246 01055000   3.49     2.09       0.167       0.306   0.599           19.2\n3 0.453 01073000   3.50     2.21       0.0304      0.175   0.630           20.8\n4 0.541 01118300   3.90     2.31      -0.0377      0.111   0.593           21.5\n5 0.420 01123000   3.82     2.31      -0.00119     0.136   0.603           21.2\n6 0.195 01142500   3.31     2.15       0.175       0.252   0.648           19.8\n# ℹ 52 more variables: high_prec_dur &lt;dbl&gt;, high_prec_timing &lt;chr&gt;,\n#   low_prec_freq &lt;dbl&gt;, low_prec_dur &lt;dbl&gt;, low_prec_timing &lt;chr&gt;,\n#   geol_1st_class &lt;chr&gt;, glim_1st_class_frac &lt;dbl&gt;, geol_2nd_class &lt;chr&gt;,\n#   glim_2nd_class_frac &lt;dbl&gt;, carbonate_rocks_frac &lt;dbl&gt;, geol_porostiy &lt;dbl&gt;,\n#   geol_permeability &lt;dbl&gt;, soil_depth_pelletier &lt;dbl&gt;,\n#   soil_depth_statsgo &lt;dbl&gt;, soil_porosity &lt;dbl&gt;, soil_conductivity &lt;dbl&gt;,\n#   max_water_content &lt;dbl&gt;, sand_frac &lt;dbl&gt;, silt_frac &lt;dbl&gt;, …\n\n\n\nggplot(augmented_data, aes(x = .pred, y = logQmean)) +\n  geom_point(aes(color = abs(.pred - logQmean)), size = 3, alpha = 0.7) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"orange\") +\n  labs(title = \"Observed vs. Predicted: Boosted Tree Model\",\n       x = \"Predicted Values\",\n       y = \"Observed Values\",\n       subtitle = \"Predictions from Boosted Tree Model on Test Data\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"blue\", high = \"green\")"
  },
  {
    "objectID": "Lab_6.html#the-results-seem-somewhat-accurate.-there-are-plenty-of-data-points-close-to-the-line-of-best-fit-there-are-some-that-are-noticably-away-from-the-line-high-residual.",
    "href": "Lab_6.html#the-results-seem-somewhat-accurate.-there-are-plenty-of-data-points-close-to-the-line-of-best-fit-there-are-some-that-are-noticably-away-from-the-line-high-residual.",
    "title": "Lab_6",
    "section": "The results seem somewhat accurate. There are plenty of data points close to the line of best fit, there are some that are noticably away from the line (high residual).",
    "text": "The results seem somewhat accurate. There are plenty of data points close to the line of best fit, there are some that are noticably away from the line (high residual)."
  }
]