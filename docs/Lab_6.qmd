---
title: "Lab_6"
author:
- name: "Aldair Perez"
email: "alpema@colostate.edu"
format: html
execute: 
  echo: true
---

```{r setup, include=FALSE}
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("powerjoin")
install.packages("vip")
install.packages("baguette")
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(purrr)
library(readr)
library(ggplot2)
library(ggthemes)
```

```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```

```{r}
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
               'data/camels_attributes_v2.0.pdf')
```

```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
```

```{r}
remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('data/camels_{types}.txt')
```

```{r}
walk2(remote_files, local_files, download.file, quiet = TRUE)
```

```{r}
camels <- map(local_files, read_delim, show_col_types = FALSE)
```

```{r}
camels <- power_full_join(camels ,by = 'gauge_id')
```

# Question 1
```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "darkgreen") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "orange", high = "blue") +
  ggthemes::theme_map()
```

# Question 2
```{r}
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()
```
```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm", color = "yellow", linetype = 2) +
  scale_color_viridis_c() +
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```

## Model Building
```{r}
set.seed(123)
camels <- camels |>
  mutate(logQmean = log(q_mean))
```

```{r}
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

```{r}
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ aridity:p_mean) |>
  step_naomit(all_predictors(), all_outcomes())
```

```{r}
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)
```
```{r}
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```

```{r}
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

```{r}
metrics(test_data, truth = logQmean, estimate = lm_pred)
```
```{r}
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  scale_color_gradient2(low = "red", mid = "orange", high = "yellow") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

```{r}
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

lm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model) %>%
  fit(data = camels_train) 

summary(extract_fit_engine(lm_wf))$coefficients
```
```{r}
summary(lm_base)$coefficients
```
```{r}
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```
```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
install.packages("ranger")
library(ranger)
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = camels_train) 
```

```{r}
rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```
```{r}
metrics(rf_data, truth = logQmean, estimate = .pred)
```
```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```
```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

# Question 3
```{r}
boost_model <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode('regression')
```

```{r}
install.packages("modeltime")
library(modeltime)
boost_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(boost_model) %>%
  fit(data = camels_train)
```

```{r}
neural_model <- bag_mlp() |>
  set_engine("nnet") |>
  set_mode('regression')
```

```{r}
neural_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(neural_model) %>%
  fit(data = camels_train)
```

## Through evaluation of the models I have made,random forest is the best to use due to its simplicity and works with multiple decision trees, as well as being accurate.

# Build Your Own
## Data Spliting
```{r}
set.seed(69)
data_splits <- initial_split(camels, prop = 0.75)
data_train <- training(data_splits)
data_test <- testing(data_splits)

data_cv <- vfold_cv(data_train, v = 10)
```

## Recipe
```{r}
rec2 <- recipe(logQmean ~ p_mean + baseflow_index, data = data_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ p_mean:baseflow_index) %>%
  step_naomit(all_predictors(), all_outcomes())
```

```{r}
camels |>
  select(p_mean, runoff_ratio, baseflow_index) |>
  drop_na() |>
  cor()
```
## I chose p_mean since mean daily precipiation can influence streamflow. Runoff_ratio is the next predictor since it helps us find out how much of the precipitation turns into streamflow. Finally, baseflow_index tells us how much of the streamflow is from groundwater.

## Define Models
```{r}
randomly_forest <- rand_forest() |>
  set_engine("ranger") |>
  set_mode("regression")
```

```{r}
boosted_model <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("regression")
```

```{r}
svm_model <- svm_poly() |>
  set_engine("kernlab") |>
  set_mode("regression")
```

## Workflow Set
```{r}
randfore_workflow <- workflow() %>%
  add_recipe(rec2) %>%
  add_model(randomly_forest) %>%
  fit(data = data_train)
```

```{r}
boosted_workflow <- workflow() %>%
  add_recipe(rec2) %>%
  add_model(boosted_model) %>%
  fit(data = data_train)
```

```{r}
install.packages("kernlab")
library(kernlab)
svm_workflow <- workflow() %>%
  add_recipe(rec2) %>%
  add_model(svm_model) %>%
  fit(data = data_train)
```

```{r}
model_set <- workflow_set(
  preproc = list("recipe" = rec2),
  models = list(
    rf = randomly_forest,                  
    boosted = boosted_model,        
    svm = svm_model))
```

## Evaluation
```{r}
randfore_resamples <- fit_resamples(randfore_workflow, resamples = data_cv)
boosted_resamples <- fit_resamples(boosted_workflow, resamples = data_cv)
svm_resamples <- fit_resamples(svm_workflow, resamples = data_cv)
```

```{r}
randfore_metrics <- collect_metrics(randfore_resamples)
boosted_metrics <- collect_metrics(boosted_resamples)
svm_metrics <- collect_metrics(svm_resamples)
```

```{r}
all_metrics <- bind_rows(
  randfore_metrics %>% mutate(model = "Random Forest"),
  boosted_metrics %>% mutate(model = "Boosted"),
  svm_metrics %>% mutate(model = "SVM")
)
```

```{r}
ranked_models <- all_metrics %>%
  arrange(mean) %>%
  select(model, mean)
```

```{r}
ggplot(ranked_models, aes(x = model, y = mean, fill = model)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparison of Model Performance (Mean Metric)", 
       y = "Mean RMSE (or another metric)",
       x = "Model") +
  theme_minimal()
```

```{r}
ranked_models %>%
     arrange(mean)
print(ranked_models)
```

## The best model is boosted because it's in the middle in mean and the difference between the 2 boosted models is the smallest.

## Extact and Evaluate
```{r}
boosted_workflow <- workflow() %>%
  add_model(boosted_model) %>%  
  add_recipe(rec2) %>%
  fit(data = data_train)
```

```{r}
augmented_data <- augment(boosted_workflow, new_data = data_test)
head(augmented_data)
```
```{r}
ggplot(augmented_data, aes(x = .pred, y = logQmean)) +
  geom_point(aes(color = abs(.pred - logQmean)), size = 3, alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "orange") +
  labs(title = "Observed vs. Predicted: Boosted Tree Model",
       x = "Predicted Values",
       y = "Observed Values",
       subtitle = "Predictions from Boosted Tree Model on Test Data") +
  theme_minimal() +
  scale_color_gradient(low = "blue", high = "green") 
```

## The results seem somewhat accurate. There are plenty of data points close to the line of best fit, there are some that are noticably away from the line (high residual).